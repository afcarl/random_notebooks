{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated MANOVA in Python\n",
    "I've worked quite a bit on 'decoding (machine-learning) analyses' of fMRI data. A while back, I came across the article by [Allefeld & Haynes (2014)](http://www.sciencedirect.com/science/article/pii/S1053811913011920) on 'cross-validated MANOVA', an multivariate encoding-type of analysis. In this notebook, I've tried to implement this analysis and annotated some of the steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGLM\n",
    "The MANOVA is a test derived from the multivariate general linear model (MGLM). Conceptually, a MANOVA tests how much variance in a set of target variables (dependent variables; $\\mathbf{y}$) by a (set of) predictor(s) ($\\mathbf{X}$); in other words, it's a \"multivariate analysis of variance\", as the name implies. In my understanding (which is often more conceptual than mathematical), the first step in the analysis described by Allefeld & Haynes is to simply calculate the parameters of the GLM ($\\beta$) for all target variables. The GLM, here, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "y = \\beta\\mathbf{X} + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "in which $\\beta$ represents the GLM's parameter(s) and $\\epsilon$ the model's errors. Now, the parameter(s) $\\beta$ can be found by the GLM's analytical solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (\\mathbf{X}'\\mathbf{X})^{-1}y\n",
    "\\end{align}\n",
    "\n",
    "In which $X$ represents the design-matrix (predictors), a $N\\ (samples) \\times P\\ (predictors)$ matrix, and $y$ represents a $N \\times 1$ column-vector with the target variable. I recently found at that you can estimate the parameters for different target variables (so different $y$ variables) corresponding to the model:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y} = \\beta\\mathbf{X} + \\Xi\n",
    "\\end{align}\n",
    "\n",
    "in which $\\mathbf{y}$ is now a $N \\times K$ (number of target variables) matrix and $\\Xi$ a $N \\times K$ matrix with the model errors. As such, the parameters of this model can be found by vectorizing the previously outlined formula for the (univariate) GLM as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "in which, now, $\\mathbf{y}$ is an $N \\times K$ (target-variables) matrix and $\\beta$ is an $P \\times K$ matrix.\n",
    "\n",
    "Let's check out an example. We'll use the 'canonical' Iris-dataset, which contains data of three types of Iris flowers (let's say, class C1, C2, and C3) associated with four types of variables (let's call them V1, V2, V3, and V4). In the context of a MANOVA, we could investigate whether the factor 'flower type' has a multivariate effect on the four measured variables. Thus, here flower type represents the design-matrix ($\\mathbf{X}$) and the measured variables represent the target variables ($\\mathbf{y}$). Note, this may seem counterintuitive for people that are familiar with this dataset in the context of machine learning, in which it is used to predict flower type ($y$) as a function of the flower features ($\\mathbf{X}$). This is, essentially, just a manifestation of a different question about the data. In neuroimaging, however, this difference is quite important (check out the awesome article by [Naselaris and colleagues](http://www.sciencedirect.com/science/article/pii/S1053811910010657) on this topic).\n",
    "\n",
    "Anyway, let's check out how the parameters are calculated in the MGLM. We'll first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y (flower properties): (150, 4)\n",
      "Shape of flower types variable: (150,)\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "y, flower_types = load_iris(return_X_y=True)\n",
    "print(\"Shape of y (flower properties): %s\" % (y.shape,))\n",
    "print(\"Shape of flower types variable: %s\" % (flower_types.shape,), end='\\n\\n')\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the effect of flower type on the target variables in the context of the GLM, we need to create a separate predictor for each class (C1, C2, C3), which contains all zeros except for the samples that correspond to that particular class. This process is known (albeit in a different context) as 'one-hot encoding'. Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (flower type): (150, 3)\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "X = ohe.fit_transform(flower_types[:, np.newaxis]).astype(float)\n",
    "print(\"Shape of X (flower type): %s\" % (X.shape,))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Exactly what we wanted. Now, let's solve the parameters of this GLM-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of betas: (3, 4)\n",
      "[[ 5.006  3.418  1.464  0.244]\n",
      " [ 5.936  2.77   4.26   1.326]\n",
      " [ 6.588  2.974  5.552  2.026]]\n"
     ]
    }
   ],
   "source": [
    "betas = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(\"Shape of betas: %s\" % (betas.shape,))\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the betas (given this design-matrix) represent the mean of each class (rows) for each variable (columns).\n",
    "\n",
    "## The MANOVA\n",
    "Now, the Allefeld & Haynes paper define a 'pattern-distinctness' statistic $\\hat{D}$, which is based on the Barlett-Lawley-Hotelling Trace $T_{BLH}$, which represents the trace of the product of the between-class covariance (I call it $B$) and the inverse of the within-class covariance (they call it $\\Sigma$). Thus, the formula for the Barlett-Lawley-Hotelling trace become:\n",
    "\n",
    "\\begin{align}\n",
    "T_{BLH} = \\mathrm{trace}(B\\Sigma^{-1})\n",
    "\\end{align}\n",
    "\n",
    "In the paper, $B$ is calculated using the following formula (this becomes a bit hairy):\n",
    "\n",
    "\\begin{align}\n",
    "B = \\hat{\\beta}_{\\Delta}'\\mathbf{X}'\\mathbf{X}\\hat{\\beta}_{\\Delta}\n",
    "\\end{align}\n",
    "\n",
    "in which $\\hat{\\beta}_{\\Delta}$ is calculated using the contrast-matrix $C$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_{\\Delta} = (C'C)^{-1}C\\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "Fortunately, $\\Sigma$ is simply the inner product of the residuals from the model:\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma = (\\mathbf{y} - \\hat{\\beta}\\mathbf{X})'(\\mathbf{y} - \\hat{\\beta}\\mathbf{X}) = \\hat{\\Xi}'\\hat{\\Xi}\n",
    "\\end{align}\n",
    "\n",
    "Alright, but what contrast should we use? Well, that of course depends on your question. In our case, we said we were interested in the effect of flower type on the target variables. Given that we have three flower types (C1, C2, C3), an F-test of $K - 1$ pairwise difference contrasts (e.g. C1 - C2 and C1 - C3) should do the trick. As such, we could define $C$ as:\n",
    "\n",
    "\\begin{align}\n",
    "C =\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & 0\\\\1 & 0 & -1\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Let's define it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [1, 0, -1],\n",
    "    [0, 1, -1]    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate $\\hat{\\beta}_{\\Delta}$ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape B-delta: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "beta_delta = C.T.dot(np.linalg.pinv(C.T.dot(C)).dot(C.T).T).dot(betas)\n",
    "print(\"Shape B-delta: %s\" % (beta_delta.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and $B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = beta_delta.T.dot(X.T.dot(X)).dot(beta_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... $\\Sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = (y - X.dot(betas)).T.dot((y - X.dot(betas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and finally $T_{BLH}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.5495246636\n"
     ]
    }
   ],
   "source": [
    "T_blh = np.trace(B.dot(np.linalg.inv(sigma)))\n",
    "print(T_blh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always check whether I got the right result by checking it against some existing implementation. Fortunately, there is a MANOVA implementation in the master branch (not yet op PyPI) of the statsmodels package. So, let's check whether it matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td> <td></td> <td></td> <td></td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td>     <th>One-way MANOVA</th>      <th>Value</th>  <th>Num DF</th>  <th>Den DF</th>   <th>F Value</th>  <th>Pr > F</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>      <td>Wilks' lambda</td>     <td>0.0235</td>  <td>8.0000</td> <td>288.0000</td> <td>198.7110</td>  <td>0.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>     <td>Pillai's trace</td>     <td>1.1872</td>  <td>8.0000</td> <td>290.0000</td>  <td>52.9486</td>  <td>0.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th> <td>Hotelling-Lawley trace</td> <td>32.5495</td> <td>8.0000</td> <td>203.4024</td> <td>583.4914</td>  <td>0.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>   <td>Roy's greatest root</td>  <td>32.2720</td> <td>4.0000</td> <td>145.0000</td> <td>1169.8585</td> <td>0.0000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Multivariate linear model\n",
       "================================================================\n",
       "                                                                \n",
       "----------------------------------------------------------------\n",
       "     One-way MANOVA      Value  Num DF  Den DF   F Value  Pr > F\n",
       "----------------------------------------------------------------\n",
       "          Wilks' lambda  0.0235 8.0000 288.0000  198.7110 0.0000\n",
       "         Pillai's trace  1.1872 8.0000 290.0000   52.9486 0.0000\n",
       " Hotelling-Lawley trace 32.5495 8.0000 203.4024  583.4914 0.0000\n",
       "    Roy's greatest root 32.2720 4.0000 145.0000 1169.8585 0.0000\n",
       "================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "mglm = MANOVA(endog=y, exog=X)\n",
    "fitted_model = mglm.mv_test([    \n",
    "    ('One-way MANOVA', C)\n",
    "])\n",
    "\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Exactly the same (the value in statsmodels is called *Hotelling-Layley trace*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *cross-validated* MANOVA\n",
    "Importantly, the paper suggests to cross-validate the MANOVA procedure (and thus the calculation of the statistic of interest, $\\hat{D}$), because $\\hat{D}$ is a distance-measure -- essentially an extension of the mahalanobis distance for more than two groups (levels) -- which are inherently positively biased because distances cannot be negative. Cross-validated makes sure the estimate becomes unbiased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.26461949482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97166666666666668"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "M = 5\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "T_blh_cv = np.zeros(5)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X=y, y=flower_types)):\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    betas_train = np.linalg.pinv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "    betas_test = np.linalg.pinv(X_test.T.dot(X_test)).dot(X_test.T).dot(y_test)\n",
    "    beta_delta_train = C.T.dot(np.linalg.pinv(C.T.dot(C)).dot(C.T).T).dot(betas_train)\n",
    "    beta_delta_test = C.T.dot(np.linalg.pinv(C.T.dot(C)).dot(C.T).T).dot(betas_test)\n",
    "    B_cv = beta_delta_train.T.dot(X_test.T.dot(X_test)).dot(beta_delta_test)\n",
    "    sigma_cv = (y_train - X_train.dot(betas_train)).T.dot((y_train - X_train.dot(betas_train)))\n",
    "    T_blh_cv[i] = np.trace(B_cv.dot(np.linalg.inv(sigma_cv)))\n",
    "\n",
    "print(T_blh_cv.mean())\n",
    "N = X.shape[0]\n",
    "((M - 1)*(N - np.linalg.matrix_rank(X))-y.shape[1]-1) / ((M - 1) * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, this is much lower than the uncrossvalidated statistic. Need to check up on this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
